#!/usr/bin/env python3
"""
eBPFæ•°æ®åˆ†æå·¥å…·ä¸»ç¨‹åº
æä¾›æ•°æ®åˆ†å‰²ã€åŠ è½½ã€åˆ†æå’Œå¯¹æ¯”åŠŸèƒ½
"""

import os
import sys
import argparse
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import logging

from data_utils import (
    scan_output_files, safe_read_csv, extract_date, 
    parse_timestamp, get_date_range_from_files
)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EBPFAnalyzer:
    """eBPFæ•°æ®åˆ†æå™¨"""
    
    def __init__(self, output_dir: str = "../output", daily_data_dir: str = "./daily_data"):
        self.output_dir = output_dir
        self.daily_data_dir = daily_data_dir
        self.monitor_types = ['exec', 'syscall', 'io', 'interrupt', 'func', 'open', 'page_fault']
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.daily_data_dir, exist_ok=True)
    
    
    def clean_loaded_data(self, df: pd.DataFrame, monitor_type: str) -> pd.DataFrame:
        """
        æ¸…ç†åŠ è½½çš„æ•°æ®ï¼Œå¤„ç†æ ¼å¼é—®é¢˜
        
        Args:
            df: åŸå§‹DataFrame
            monitor_type: ç›‘æ§å™¨ç±»å‹
            
        Returns:
            æ¸…ç†åçš„DataFrame
        """
        if df.empty:
            return df
        
        original_count = len(df)
        
        # 1. ç§»é™¤å®Œå…¨ç©ºçš„è¡Œ
        df = df.dropna(how='all')
        
        # 2. å¤„ç†timestampåˆ—
        if 'timestamp' in df.columns:
            # ç§»é™¤æ— æ•ˆçš„timestamp
            df = df.dropna(subset=['timestamp'])
            
            # å°è¯•è½¬æ¢timestampä¸ºæ•°å€¼ç±»å‹
            def safe_convert_timestamp(ts):
                try:
                    if pd.isna(ts) or ts == '':
                        return None
                    # å°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                    return float(str(ts).strip())
                except:
                    return None
            
            df['timestamp'] = df['timestamp'].apply(safe_convert_timestamp)
            df = df.dropna(subset=['timestamp'])
        
        # 3. å¤„ç†å…¶ä»–æ•°å€¼åˆ—
        numeric_columns = {
            'exec': ['uid', 'pid', 'ppid', 'ret'],
            'syscall': ['pid', 'tid', 'cpu', 'syscall_nr', 'ret_val', 'duration_ns', 'duration_us', 'duration_ms'],
            'io': ['io_type', 'fd', 'size', 'duration_ns', 'duration_us', 'throughput_mbps', 'pid', 'tid', 'cpu', 'ret_val'],
            'interrupt': ['irq_num', 'irq_type', 'duration_ns', 'duration_us', 'cpu', 'softirq_vec', 'pid', 'tid'],
            'func': ['pid', 'ppid', 'uid'],
            'open': ['type', 'pid', 'tid', 'uid', 'cpu', 'flags', 'mode', 'ret'],
            'page_fault': ['pid', 'tid', 'address', 'fault_type', 'cpu']
        }
        
        if monitor_type in numeric_columns:
            for col in numeric_columns[monitor_type]:
                if col in df.columns:
                    def safe_convert_numeric(val):
                        try:
                            if pd.isna(val) or val == '':
                                return 0
                            return pd.to_numeric(str(val).strip(), errors='coerce')
                        except:
                            return 0
                    
                    df[col] = df[col].apply(safe_convert_numeric)
        
        # 4. å¤„ç†å¸ƒå°”åˆ—
        boolean_columns = {
            'syscall': ['is_error', 'is_slow_call'],
            'io': ['is_error'],
            'page_fault': ['is_major_fault', 'is_minor_fault', 'is_write_fault', 'is_user_fault']
        }
        
        if monitor_type in boolean_columns:
            for col in boolean_columns[monitor_type]:
                if col in df.columns:
                    def safe_convert_boolean(val):
                        try:
                            if pd.isna(val) or val == '':
                                return False
                            val_str = str(val).strip().lower()
                            return val_str in ['true', '1', 'yes', 'on']
                        except:
                            return False
                    
                    df[col] = df[col].apply(safe_convert_boolean)
        
        # 5. æ¸…ç†å­—ç¬¦ä¸²åˆ—ï¼Œç§»é™¤å¼•å·å’Œç‰¹æ®Šå­—ç¬¦
        string_columns = df.select_dtypes(include=['object']).columns
        for col in string_columns:
            if col not in ['timestamp']:  # è·³è¿‡å·²å¤„ç†çš„åˆ—
                def clean_string(val):
                    try:
                        if pd.isna(val):
                            return ''
                        val_str = str(val).strip()
                        # ç§»é™¤é¦–å°¾çš„å¼•å·
                        if val_str.startswith('"') and val_str.endswith('"'):
                            val_str = val_str[1:-1]
                        # å¤„ç†è½¬ä¹‰çš„å¼•å·
                        val_str = val_str.replace('""', '"')
                        return val_str
                    except:
                        return ''
                
                df[col] = df[col].apply(clean_string)
        
        cleaned_count = len(df)
        if original_count != cleaned_count:
            logger.info(f"{monitor_type} æ•°æ®æ¸…ç†: {original_count} -> {cleaned_count} è¡Œ")
        
        return df
    
    def load_daily_data(self, date: str, monitor_types: Optional[List[str]] = None) -> Dict[str, pd.DataFrame]:
        """
        åŠ è½½æŒ‡å®šæ—¥æœŸçš„æ•°æ®
        
        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸² (YYYYMMDDæ ¼å¼)
            monitor_types: ç›‘æ§å™¨ç±»å‹åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºåŠ è½½æ‰€æœ‰ç±»å‹
            
        Returns:
            æŒ‰ç›‘æ§å™¨ç±»å‹åˆ†ç»„çš„DataFrameå­—å…¸
        """
        if monitor_types is None:
            monitor_types = self.monitor_types
        
        data = {}
        
        for monitor_type in monitor_types:
            filepath = os.path.join(self.daily_data_dir, f"{monitor_type}_{date}.csv")
            if os.path.exists(filepath):
                try:
                    df = safe_read_csv(filepath)
                    if not df.empty:
                        # æ¸…ç†æ•°æ®
                        df = self.clean_loaded_data(df, monitor_type)
                        if not df.empty:
                            data[monitor_type] = df
                            logger.info(f"åŠ è½½ {monitor_type} æ•°æ®: {len(df)} è¡Œ")
                        else:
                            logger.warning(f"æ•°æ®æ¸…ç†åä¸ºç©º: {filepath}")
                    else:
                        logger.warning(f"æ–‡ä»¶ä¸ºç©º: {filepath}")
                except Exception as e:
                    logger.error(f"åŠ è½½æ•°æ®å¤±è´¥ {filepath}: {e}")
            else:
                logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        
        return data
    
    def load_date_range(self, start_date: str, end_date: str, 
                       monitor_types: Optional[List[str]] = None) -> Dict[str, Dict[str, pd.DataFrame]]:
        """
        åŠ è½½æ—¥æœŸèŒƒå›´å†…çš„æ•°æ®
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYYMMDDæ ¼å¼)
            end_date: ç»“æŸæ—¥æœŸ (YYYYMMDDæ ¼å¼)
            monitor_types: ç›‘æ§å™¨ç±»å‹åˆ—è¡¨
            
        Returns:
            æŒ‰æ—¥æœŸå’Œç›‘æ§å™¨ç±»å‹åˆ†ç»„çš„åµŒå¥—å­—å…¸
        """
        data_by_date = {}
        
        start_dt = datetime.strptime(start_date, '%Y%m%d')
        end_dt = datetime.strptime(end_date, '%Y%m%d')
        
        current_dt = start_dt
        while current_dt <= end_dt:
            date_str = current_dt.strftime('%Y%m%d')
            daily_data = self.load_daily_data(date_str, monitor_types)
            if daily_data:
                data_by_date[date_str] = daily_data
            current_dt += timedelta(days=1)
        
        return data_by_date
    
    def analyze_performance(self, data: Dict[str, pd.DataFrame]) -> Dict[str, Dict]:
        """
        åˆ†æç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        
        Args:
            data: æŒ‰ç›‘æ§å™¨ç±»å‹åˆ†ç»„çš„æ•°æ®
            
        Returns:
            æ€§èƒ½åˆ†æç»“æœå­—å…¸
        """
        results = {}
        
        # åˆ†æç³»ç»Ÿè°ƒç”¨æ€§èƒ½
        if 'syscall' in data:
            syscall_df = data['syscall']
            if not syscall_df.empty and 'duration_ms' in syscall_df.columns:
                results['syscall'] = {
                    'total_calls': len(syscall_df),
                    'avg_duration_ms': syscall_df['duration_ms'].mean(),
                    'max_duration_ms': syscall_df['duration_ms'].max(),
                    'error_rate': (syscall_df['is_error'] == True).sum() / len(syscall_df) if 'is_error' in syscall_df.columns else 0,
                    'slow_calls': (syscall_df['is_slow_call'] == True).sum() if 'is_slow_call' in syscall_df.columns else 0
                }
        
        # åˆ†æI/Oæ€§èƒ½
        if 'io' in data:
            io_df = data['io']
            if not io_df.empty:
                results['io'] = {
                    'total_operations': len(io_df),
                    'avg_throughput_mbps': io_df['throughput_mbps'].mean() if 'throughput_mbps' in io_df.columns else 0,
                    'avg_duration_us': io_df['duration_us'].mean() if 'duration_us' in io_df.columns else 0,
                    'read_operations': len(io_df[io_df['type_str'] == 'READ']) if 'type_str' in io_df.columns else 0,
                    'write_operations': len(io_df[io_df['type_str'] == 'WRITE']) if 'type_str' in io_df.columns else 0
                }
        
        # åˆ†æè¿›ç¨‹æ‰§è¡Œ
        if 'exec' in data:
            exec_df = data['exec']
            if not exec_df.empty:
                results['exec'] = {
                    'total_processes': len(exec_df),
                    'unique_commands': exec_df['comm'].nunique() if 'comm' in exec_df.columns else 0,
                    'failed_executions': (exec_df['ret'] != 0).sum() if 'ret' in exec_df.columns else 0
                }
                
                # åˆ†æfilenameå­—æ®µåˆ†å¸ƒ
                if 'filename' in exec_df.columns:
                    filename_counts = exec_df['filename'].value_counts()
                    results['exec']['filename_distribution'] = filename_counts.head(10).to_dict()
                
                # åˆ†æargvå­—æ®µåˆ†å¸ƒï¼ˆå¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„ï¼‰
                if 'argv' in exec_df.columns:
                    # æå–å¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„ï¼ˆargvçš„ç¬¬ä¸€éƒ¨åˆ†ï¼‰
                    exec_paths = exec_df['argv'].str.split().str[0].value_counts()
                    results['exec']['executable_distribution'] = exec_paths.head(10).to_dict()
        
        # åˆ†æä¸­æ–­
        if 'interrupt' in data:
            interrupt_df = data['interrupt']
            if not interrupt_df.empty:
                results['interrupt'] = {
                    'total_interrupts': len(interrupt_df),
                    'avg_duration_us': interrupt_df['duration_us'].mean() if 'duration_us' in interrupt_df.columns else 0,
                    'hardware_interrupts': len(interrupt_df[interrupt_df['irq_type_str'].str.contains('HARDWARE', na=False)]) if 'irq_type_str' in interrupt_df.columns else 0,
                    'software_interrupts': len(interrupt_df[interrupt_df['irq_type_str'].str.contains('SOFTWARE', na=False)]) if 'irq_type_str' in interrupt_df.columns else 0
                }
        
        # åˆ†æé¡µé¢é”™è¯¯
        if 'page_fault' in data:
            pf_df = data['page_fault']
            if not pf_df.empty:
                results['page_fault'] = {
                    'total_faults': len(pf_df),
                    'major_faults': (pf_df['is_major_fault'] == True).sum() if 'is_major_fault' in pf_df.columns else 0,
                    'minor_faults': (pf_df['is_minor_fault'] == True).sum() if 'is_minor_fault' in pf_df.columns else 0,
                    'write_faults': (pf_df['is_write_fault'] == True).sum() if 'is_write_fault' in pf_df.columns else 0
                }
        
        return results
    
    def compare_systems(self, dates: List[str], monitor_types: Optional[List[str]] = None) -> Dict[str, Dict]:
        """
        å¯¹æ¯”ä¸åŒæ—¥æœŸçš„ç³»ç»Ÿæ€§èƒ½
        
        Args:
            dates: æ—¥æœŸåˆ—è¡¨
            monitor_types: ç›‘æ§å™¨ç±»å‹åˆ—è¡¨
            
        Returns:
            å¯¹æ¯”ç»“æœå­—å…¸
        """
        comparison_results = {}
        
        for date in dates:
            logger.info(f"åˆ†ææ—¥æœŸ: {date}")
            daily_data = self.load_daily_data(date, monitor_types)
            if daily_data:
                performance = self.analyze_performance(daily_data)
                comparison_results[date] = performance
            else:
                logger.warning(f"æ—¥æœŸ {date} æ— å¯ç”¨æ•°æ®")
        
        return comparison_results
    
    def detect_anomalies(self, data: Dict[str, pd.DataFrame], threshold_std: float = 3.0) -> Dict[str, List]:
        """
        æ£€æµ‹æ€§èƒ½å¼‚å¸¸
        
        Args:
            data: æ•°æ®å­—å…¸
            threshold_std: å¼‚å¸¸æ£€æµ‹é˜ˆå€¼ï¼ˆæ ‡å‡†å·®å€æ•°ï¼‰
            
        Returns:
            å¼‚å¸¸æ£€æµ‹ç»“æœ
        """
        anomalies = {}
        
        # æ£€æµ‹ç³»ç»Ÿè°ƒç”¨å¼‚å¸¸
        if 'syscall' in data:
            syscall_df = data['syscall']
            if not syscall_df.empty and 'duration_ms' in syscall_df.columns:
                mean_duration = syscall_df['duration_ms'].mean()
                std_duration = syscall_df['duration_ms'].std()
                threshold = mean_duration + threshold_std * std_duration
                
                anomaly_calls = syscall_df[syscall_df['duration_ms'] > threshold]
                if not anomaly_calls.empty:
                    anomalies['syscall_duration'] = anomaly_calls.to_dict('records')
        
        # æ£€æµ‹I/Oå¼‚å¸¸
        if 'io' in data:
            io_df = data['io']
            if not io_df.empty and 'duration_us' in io_df.columns:
                mean_duration = io_df['duration_us'].mean()
                std_duration = io_df['duration_us'].std()
                threshold = mean_duration + threshold_std * std_duration
                
                anomaly_io = io_df[io_df['duration_us'] > threshold]
                if not anomaly_io.empty:
                    anomalies['io_duration'] = anomaly_io.to_dict('records')
        
        return anomalies
    
    def get_available_dates(self) -> List[str]:
        """
        è·å–daily_dataç›®å½•ä¸­å¯ç”¨çš„æ—¥æœŸåˆ—è¡¨
        
        Returns:
            æ—¥æœŸåˆ—è¡¨ï¼Œæ ¼å¼ä¸ºYYYYMMDD
        """
        dates = set()
        
        if os.path.exists(self.daily_data_dir):
            for filename in os.listdir(self.daily_data_dir):
                if filename.endswith('.csv'):
                    # æå–æ—¥æœŸéƒ¨åˆ†
                    parts = filename.replace('.csv', '').split('_')
                    if len(parts) >= 2:
                        date_part = parts[-1]
                        if len(date_part) == 8 and date_part.isdigit():
                            dates.add(date_part)
        
        return sorted(list(dates))
    
    def analyze_exec_details(self, data: Dict[str, pd.DataFrame]) -> Dict:
        """
        è¯¦ç»†åˆ†æexecæ•°æ®
        
        Args:
            data: æ•°æ®å­—å…¸
            
        Returns:
            è¯¦ç»†åˆ†æç»“æœ
        """
        if 'exec' not in data:
            return {}
        
        exec_df = data['exec']
        if exec_df.empty:
            return {}
        
        analysis = {}
        
        # filenameå­—æ®µåˆ†æ
        if 'filename' in exec_df.columns:
            filename_series = exec_df['filename'].dropna()
            if not filename_series.empty:
                filename_counts = filename_series.value_counts()
                analysis['filename_analysis'] = {
                    'total_with_filename': len(filename_series),
                    'unique_filenames': len(filename_counts),
                    'top_filenames': filename_counts.head(20).to_dict(),
                    'empty_filenames': (exec_df['filename'].isna() | (exec_df['filename'] == '')).sum()
                }
        
        # argvå­—æ®µåˆ†æï¼ˆå¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„ï¼‰
        if 'argv' in exec_df.columns:
            argv_series = exec_df['argv'].dropna()
            if not argv_series.empty:
                # æå–å¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„
                exec_paths = argv_series.str.split().str[0]
                exec_path_counts = exec_paths.value_counts()
                analysis['executable_analysis'] = {
                    'total_executions': len(exec_paths),
                    'unique_executables': len(exec_path_counts),
                    'top_executables': exec_path_counts.head(20).to_dict()
                }
        
        # commå­—æ®µåˆ†æ
        if 'comm' in exec_df.columns:
            comm_counts = exec_df['comm'].value_counts()
            analysis['command_analysis'] = {
                'unique_commands': len(comm_counts),
                'top_commands': comm_counts.head(20).to_dict()
            }
        
        # ç”¨æˆ·åˆ†æ
        if 'uid' in exec_df.columns:
            uid_counts = exec_df['uid'].value_counts()
            analysis['user_analysis'] = {
                'unique_users': len(uid_counts),
                'executions_by_uid': uid_counts.to_dict()
            }
        
        # å¤±è´¥åˆ†æ
        if 'ret' in exec_df.columns:
            failed_df = exec_df[exec_df['ret'] != 0]
            if not failed_df.empty:
                analysis['failure_analysis'] = {
                    'total_failures': len(failed_df),
                    'failure_rate': len(failed_df) / len(exec_df),
                    'failed_commands': failed_df['comm'].value_counts().head(10).to_dict() if 'comm' in failed_df.columns else {}
                }
        
        return analysis
    
    def analyze_open_details(self, data: Dict[str, pd.DataFrame]) -> Dict:
        """
        è¯¦ç»†åˆ†æopenæ•°æ®
        
        Args:
            data: æ•°æ®å­—å…¸
            
        Returns:
            è¯¦ç»†åˆ†æç»“æœ
        """
        if 'open' not in data:
            return {}
        
        open_df = data['open']
        if open_df.empty:
            return {}
        
        analysis = {}
        
        # filenameå­—æ®µåˆ†æ
        if 'filename' in open_df.columns:
            filename_series = open_df['filename'].dropna()
            if not filename_series.empty:
                filename_counts = filename_series.value_counts()
                analysis['filename_analysis'] = {
                    'total_with_filename': len(filename_series),
                    'unique_filenames': len(filename_counts),
                    'top_filenames': filename_counts.head(20).to_dict(),
                    'empty_filenames': (open_df['filename'].isna() | (open_df['filename'] == '')).sum()
                }
                
                # æŒ‰æ–‡ä»¶æ‰©å±•ååˆ†æ
                extensions = filename_series.str.extract(r'\.([^./]+)$')[0].dropna()
                if not extensions.empty:
                    ext_counts = extensions.value_counts()
                    analysis['filename_analysis']['file_extensions'] = ext_counts.head(10).to_dict()
                
                # æŒ‰ç›®å½•åˆ†æ
                directories = filename_series.str.extract(r'^(/[^/]*(?:/[^/]*)*)/')[0].dropna()
                if not directories.empty:
                    dir_counts = directories.value_counts()
                    analysis['filename_analysis']['top_directories'] = dir_counts.head(10).to_dict()
        
        # commå­—æ®µåˆ†æ
        if 'comm' in open_df.columns:
            comm_counts = open_df['comm'].value_counts()
            analysis['command_analysis'] = {
                'unique_commands': len(comm_counts),
                'top_commands': comm_counts.head(20).to_dict()
            }
        
        # type_strå­—æ®µåˆ†æ
        if 'type_str' in open_df.columns:
            type_counts = open_df['type_str'].value_counts()
            analysis['operation_type_analysis'] = {
                'unique_types': len(type_counts),
                'type_distribution': type_counts.to_dict()
            }
        
        # ç”¨æˆ·åˆ†æ
        if 'uid' in open_df.columns:
            uid_counts = open_df['uid'].value_counts()
            analysis['user_analysis'] = {
                'unique_users': len(uid_counts),
                'operations_by_uid': uid_counts.to_dict()
            }
        
        # å¤±è´¥åˆ†æ
        if 'ret' in open_df.columns:
            failed_df = open_df[open_df['ret'] < 0]  # è´Ÿæ•°è¡¨ç¤ºå¤±è´¥
            if not failed_df.empty:
                analysis['failure_analysis'] = {
                    'total_failures': len(failed_df),
                    'failure_rate': len(failed_df) / len(open_df),
                    'failed_commands': failed_df['comm'].value_counts().head(10).to_dict() if 'comm' in failed_df.columns else {},
                    'failed_files': failed_df['filename'].value_counts().head(10).to_dict() if 'filename' in failed_df.columns else {}
                }
        
        # æƒé™åˆ†æ
        if 'flags' in open_df.columns:
            flags_counts = open_df['flags'].value_counts()
            analysis['flags_analysis'] = {
                'unique_flags': len(flags_counts),
                'top_flags': flags_counts.head(10).to_dict()
            }
        
        return analysis
    
    def analyze_func_details(self, data: Dict[str, pd.DataFrame]) -> Dict:
        """è¯¦ç»†åˆ†æfuncæ•°æ®"""
        if 'func' not in data:
            return {}
        
        func_df = data['func']
        if func_df.empty:
            return {}
        
        analysis = {}
        
        # func_nameå­—æ®µåˆ†æ
        if 'func_name' in func_df.columns:
            func_counts = func_df['func_name'].value_counts()
            analysis['function_analysis'] = {
                'unique_functions': len(func_counts),
                'top_functions': func_counts.head(20).to_dict(),
                'total_calls': len(func_df)
            }
            
            # æŒ‰å‡½æ•°ç±»å‹åˆ†ç±»
            vfs_funcs = func_df[func_df['func_name'].str.startswith('vfs_', na=False)]
            sys_funcs = func_df[func_df['func_name'].str.startswith('sys_', na=False)]
            analysis['function_analysis']['vfs_calls'] = len(vfs_funcs)
            analysis['function_analysis']['sys_calls'] = len(sys_funcs)
        
        # commå­—æ®µåˆ†æ
        if 'comm' in func_df.columns:
            comm_counts = func_df['comm'].value_counts()
            analysis['command_analysis'] = {
                'unique_commands': len(comm_counts),
                'top_commands': comm_counts.head(20).to_dict()
            }
        
        # ç”¨æˆ·åˆ†æ
        if 'uid' in func_df.columns:
            uid_counts = func_df['uid'].value_counts()
            analysis['user_analysis'] = {
                'unique_users': len(uid_counts),
                'calls_by_uid': uid_counts.to_dict()
            }
        
        return analysis
    
    def analyze_interrupt_details(self, data: Dict[str, pd.DataFrame]) -> Dict:
        """è¯¦ç»†åˆ†æinterruptæ•°æ®"""
        if 'interrupt' not in data:
            return {}
        
        interrupt_df = data['interrupt']
        if interrupt_df.empty:
            return {}
        
        analysis = {}
        
        # irq_type_strå­—æ®µåˆ†æ
        if 'irq_type_str' in interrupt_df.columns:
            type_counts = interrupt_df['irq_type_str'].value_counts()
            analysis['interrupt_type_analysis'] = {
                'unique_types': len(type_counts),
                'type_distribution': type_counts.to_dict()
            }
        
        # irq_nameå­—æ®µåˆ†æ
        if 'irq_name' in interrupt_df.columns:
            name_counts = interrupt_df['irq_name'].value_counts()
            analysis['interrupt_name_analysis'] = {
                'unique_names': len(name_counts),
                'top_interrupts': name_counts.head(20).to_dict()
            }
        
        # æŒç»­æ—¶é—´åˆ†æ
        if 'duration_us' in interrupt_df.columns:
            duration_stats = interrupt_df['duration_us'].describe()
            analysis['duration_analysis'] = {
                'avg_duration_us': duration_stats['mean'],
                'max_duration_us': duration_stats['max'],
                'min_duration_us': duration_stats['min'],
                'std_duration_us': duration_stats['std']
            }
        
        # CPUåˆ†æ
        if 'cpu' in interrupt_df.columns:
            cpu_counts = interrupt_df['cpu'].value_counts()
            analysis['cpu_analysis'] = {
                'interrupts_by_cpu': cpu_counts.to_dict()
            }
        
        # commå­—æ®µåˆ†æ
        if 'comm' in interrupt_df.columns:
            comm_counts = interrupt_df['comm'].value_counts()
            analysis['command_analysis'] = {
                'unique_commands': len(comm_counts),
                'top_commands': comm_counts.head(20).to_dict()
            }
        
        return analysis
    
    def analyze_io_details(self, data: Dict[str, pd.DataFrame]) -> Dict:
        """è¯¦ç»†åˆ†æioæ•°æ®"""
        if 'io' not in data:
            return {}
        
        io_df = data['io']
        if io_df.empty:
            return {}
        
        analysis = {}
        
        # type_strå­—æ®µåˆ†æ
        if 'type_str' in io_df.columns:
            type_counts = io_df['type_str'].value_counts()
            analysis['io_type_analysis'] = {
                'type_distribution': type_counts.to_dict()
            }
        
        # æ€§èƒ½åˆ†æ
        if 'throughput_mbps' in io_df.columns:
            throughput_stats = io_df['throughput_mbps'].describe()
            analysis['performance_analysis'] = {
                'avg_throughput_mbps': throughput_stats['mean'],
                'max_throughput_mbps': throughput_stats['max'],
                'min_throughput_mbps': throughput_stats['min']
            }
        
        if 'duration_us' in io_df.columns:
            duration_stats = io_df['duration_us'].describe()
            analysis['performance_analysis'].update({
                'avg_duration_us': duration_stats['mean'],
                'max_duration_us': duration_stats['max'],
                'min_duration_us': duration_stats['min']
            })
        
        # æ–‡ä»¶æè¿°ç¬¦åˆ†æ
        if 'fd' in io_df.columns:
            fd_counts = io_df['fd'].value_counts()
            analysis['fd_analysis'] = {
                'unique_fds': len(fd_counts),
                'top_fds': fd_counts.head(10).to_dict()
            }
        
        # å¤§å°åˆ†æ
        if 'size' in io_df.columns:
            size_stats = io_df['size'].describe()
            analysis['size_analysis'] = {
                'avg_size_bytes': size_stats['mean'],
                'max_size_bytes': size_stats['max'],
                'total_bytes': io_df['size'].sum()
            }
        
        # commå­—æ®µåˆ†æ
        if 'comm' in io_df.columns:
            comm_counts = io_df['comm'].value_counts()
            analysis['command_analysis'] = {
                'unique_commands': len(comm_counts),
                'top_commands': comm_counts.head(20).to_dict()
            }
        
        # é”™è¯¯åˆ†æ
        if 'is_error' in io_df.columns:
            error_count = (io_df['is_error'] == True).sum()
            analysis['error_analysis'] = {
                'total_errors': error_count,
                'error_rate': error_count / len(io_df)
            }
        
        return analysis
    
    def analyze_page_fault_details(self, data: Dict[str, pd.DataFrame]) -> Dict:
        """è¯¦ç»†åˆ†æpage_faultæ•°æ®"""
        if 'page_fault' not in data:
            return {}
        
        pf_df = data['page_fault']
        if pf_df.empty:
            return {}
        
        analysis = {}
        
        # fault_type_strå­—æ®µåˆ†æ
        if 'fault_type_str' in pf_df.columns:
            type_counts = pf_df['fault_type_str'].value_counts()
            analysis['fault_type_analysis'] = {
                'unique_types': len(type_counts),
                'type_distribution': type_counts.to_dict()
            }
        
        # é”™è¯¯ç±»å‹åˆ†æ
        fault_types = ['is_major_fault', 'is_minor_fault', 'is_write_fault', 'is_user_fault']
        for fault_type in fault_types:
            if fault_type in pf_df.columns:
                count = (pf_df[fault_type] == True).sum()
                if 'fault_breakdown' not in analysis:
                    analysis['fault_breakdown'] = {}
                analysis['fault_breakdown'][fault_type] = count
        
        # åœ°å€åˆ†æ
        if 'address' in pf_df.columns:
            # åˆ†æåœ°å€èŒƒå›´
            address_stats = pf_df['address'].describe()
            analysis['address_analysis'] = {
                'min_address': int(address_stats['min']),
                'max_address': int(address_stats['max']),
                'unique_addresses': pf_df['address'].nunique()
            }
        
        # commå­—æ®µåˆ†æ
        if 'comm' in pf_df.columns:
            comm_counts = pf_df['comm'].value_counts()
            analysis['command_analysis'] = {
                'unique_commands': len(comm_counts),
                'top_commands': comm_counts.head(20).to_dict()
            }
        
        # CPUåˆ†æ
        if 'cpu' in pf_df.columns:
            cpu_counts = pf_df['cpu'].value_counts()
            analysis['cpu_analysis'] = {
                'faults_by_cpu': cpu_counts.to_dict()
            }
        
        return analysis
    
    def analyze_syscall_details(self, data: Dict[str, pd.DataFrame]) -> Dict:
        """è¯¦ç»†åˆ†æsyscallæ•°æ®"""
        if 'syscall' not in data:
            return {}
        
        syscall_df = data['syscall']
        if syscall_df.empty:
            return {}
        
        analysis = {}
        
        # syscall_nameå­—æ®µåˆ†æ
        if 'syscall_name' in syscall_df.columns:
            syscall_counts = syscall_df['syscall_name'].value_counts()
            analysis['syscall_analysis'] = {
                'unique_syscalls': len(syscall_counts),
                'top_syscalls': syscall_counts.head(20).to_dict(),
                'total_calls': len(syscall_df)
            }
        
        # categoryå­—æ®µåˆ†æ
        if 'category' in syscall_df.columns:
            category_counts = syscall_df['category'].value_counts()
            analysis['category_analysis'] = {
                'unique_categories': len(category_counts),
                'category_distribution': category_counts.to_dict()
            }
        
        # æ€§èƒ½åˆ†æ
        if 'duration_ms' in syscall_df.columns:
            duration_stats = syscall_df['duration_ms'].describe()
            analysis['performance_analysis'] = {
                'avg_duration_ms': duration_stats['mean'],
                'max_duration_ms': duration_stats['max'],
                'min_duration_ms': duration_stats['min'],
                'std_duration_ms': duration_stats['std']
            }
        
        # é”™è¯¯åˆ†æ
        if 'is_error' in syscall_df.columns:
            error_count = (syscall_df['is_error'] == True).sum()
            analysis['error_analysis'] = {
                'total_errors': error_count,
                'error_rate': error_count / len(syscall_df)
            }
            
            if 'error_name' in syscall_df.columns:
                error_df = syscall_df[syscall_df['is_error'] == True]
                if not error_df.empty:
                    error_names = error_df['error_name'].value_counts()
                    analysis['error_analysis']['error_types'] = error_names.to_dict()
        
        # æ…¢è°ƒç”¨åˆ†æ
        if 'is_slow_call' in syscall_df.columns:
            slow_count = (syscall_df['is_slow_call'] == True).sum()
            analysis['slow_call_analysis'] = {
                'total_slow_calls': slow_count,
                'slow_call_rate': slow_count / len(syscall_df)
            }
        
        # commå­—æ®µåˆ†æ
        if 'comm' in syscall_df.columns:
            comm_counts = syscall_df['comm'].value_counts()
            analysis['command_analysis'] = {
                'unique_commands': len(comm_counts),
                'top_commands': comm_counts.head(20).to_dict()
            }
        
        return analysis
    
    def print_summary(self, results: Dict) -> None:
        """
        æ‰“å°åˆ†æç»“æœæ‘˜è¦
        
        Args:
            results: åˆ†æç»“æœå­—å…¸
        """
        print("\n" + "="*60)
        print("eBPF ç³»ç»Ÿæ€§èƒ½åˆ†ææŠ¥å‘Š")
        print("="*60)
        
        for date, performance in results.items():
            print(f"\næ—¥æœŸ: {date}")
            print("-" * 40)
            
            for monitor_type, metrics in performance.items():
                print(f"\n{monitor_type.upper()} ç›‘æ§å™¨:")
                for metric, value in metrics.items():
                    if isinstance(value, dict) and metric.endswith('_distribution'):
                        print(f"  {metric}:")
                        for k, v in list(value.items())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                            print(f"    {k}: {v}")
                        if len(value) > 5:
                            print(f"    ... è¿˜æœ‰ {len(value) - 5} é¡¹")
                    elif isinstance(value, float):
                        print(f"  {metric}: {value:.4f}")
                    else:
                        print(f"  {metric}: {value}")
    
    def print_exec_details(self, exec_analysis: Dict) -> None:
        """
        æ‰“å°execè¯¦ç»†åˆ†æç»“æœ
        
        Args:
            exec_analysis: execåˆ†æç»“æœ
        """
        print("\n" + "="*60)
        print("EXEC ç›‘æ§å™¨è¯¦ç»†åˆ†ææŠ¥å‘Š")
        print("="*60)
        
        if 'filename_analysis' in exec_analysis:
            fa = exec_analysis['filename_analysis']
            print(f"\nğŸ“ FILENAME å­—æ®µåˆ†æ:")
            print(f"  åŒ…å«filenameçš„è®°å½•æ•°: {fa['total_with_filename']}")
            print(f"  ç©ºfilenameè®°å½•æ•°: {fa['empty_filenames']}")
            print(f"  å”¯ä¸€filenameæ•°é‡: {fa['unique_filenames']}")
            print(f"  å‰20ä¸ªæœ€å¸¸è§çš„filename:")
            for i, (filename, count) in enumerate(fa['top_filenames'].items(), 1):
                print(f"    {i:2d}. {filename or '(ç©º)'}: {count} æ¬¡")
        
        if 'executable_analysis' in exec_analysis:
            ea = exec_analysis['executable_analysis']
            print(f"\nğŸš€ å¯æ‰§è¡Œæ–‡ä»¶åˆ†æ:")
            print(f"  æ€»æ‰§è¡Œæ¬¡æ•°: {ea['total_executions']}")
            print(f"  å”¯ä¸€å¯æ‰§è¡Œæ–‡ä»¶æ•°: {ea['unique_executables']}")
            print(f"  å‰20ä¸ªæœ€å¸¸æ‰§è¡Œçš„ç¨‹åº:")
            for i, (exe, count) in enumerate(ea['top_executables'].items(), 1):
                print(f"    {i:2d}. {exe}: {count} æ¬¡")
        
        if 'command_analysis' in exec_analysis:
            ca = exec_analysis['command_analysis']
            print(f"\nğŸ’» å‘½ä»¤åˆ†æ:")
            print(f"  å”¯ä¸€å‘½ä»¤æ•°: {ca['unique_commands']}")
            print(f"  å‰20ä¸ªæœ€å¸¸è§çš„å‘½ä»¤:")
            for i, (cmd, count) in enumerate(ca['top_commands'].items(), 1):
                print(f"    {i:2d}. {cmd}: {count} æ¬¡")
        
        if 'user_analysis' in exec_analysis:
            ua = exec_analysis['user_analysis']
            print(f"\nğŸ‘¤ ç”¨æˆ·åˆ†æ:")
            print(f"  æ¶‰åŠç”¨æˆ·æ•°: {ua['unique_users']}")
            print(f"  å„ç”¨æˆ·æ‰§è¡Œæ¬¡æ•°:")
            for uid, count in ua['executions_by_uid'].items():
                user_name = "root" if uid == 0 else f"uid_{uid}"
                print(f"    {user_name}: {count} æ¬¡")
        
        if 'failure_analysis' in exec_analysis:
            fa = exec_analysis['failure_analysis']
            print(f"\nâŒ å¤±è´¥åˆ†æ:")
            print(f"  å¤±è´¥æ¬¡æ•°: {fa['total_failures']}")
            print(f"  å¤±è´¥ç‡: {fa['failure_rate']:.2%}")
            if fa['failed_commands']:
                print(f"  å¤±è´¥æœ€å¤šçš„å‘½ä»¤:")
                for cmd, count in fa['failed_commands'].items():
                    print(f"    {cmd}: {count} æ¬¡")
    
    def print_open_details(self, open_analysis: Dict) -> None:
        """
        æ‰“å°openè¯¦ç»†åˆ†æç»“æœ
        
        Args:
            open_analysis: openåˆ†æç»“æœ
        """
        print("\n" + "="*60)
        print("OPEN ç›‘æ§å™¨è¯¦ç»†åˆ†ææŠ¥å‘Š")
        print("="*60)
        
        if 'filename_analysis' in open_analysis:
            fa = open_analysis['filename_analysis']
            print(f"\nğŸ“ FILENAME å­—æ®µåˆ†æ:")
            print(f"  åŒ…å«filenameçš„è®°å½•æ•°: {fa['total_with_filename']}")
            print(f"  ç©ºfilenameè®°å½•æ•°: {fa['empty_filenames']}")
            print(f"  å”¯ä¸€filenameæ•°é‡: {fa['unique_filenames']}")
            print(f"  å‰20ä¸ªæœ€å¸¸è®¿é—®çš„æ–‡ä»¶:")
            for i, (filename, count) in enumerate(fa['top_filenames'].items(), 1):
                print(f"    {i:2d}. {filename or '(ç©º)'}: {count} æ¬¡")
            
            if 'file_extensions' in fa:
                print(f"\n  ğŸ“„ æ–‡ä»¶æ‰©å±•ååˆ†å¸ƒ:")
                for i, (ext, count) in enumerate(fa['file_extensions'].items(), 1):
                    print(f"    {i:2d}. .{ext}: {count} æ¬¡")
            
            if 'top_directories' in fa:
                print(f"\n  ğŸ“‚ æœ€å¸¸è®¿é—®çš„ç›®å½•:")
                for i, (directory, count) in enumerate(fa['top_directories'].items(), 1):
                    print(f"    {i:2d}. {directory}: {count} æ¬¡")
        
        if 'command_analysis' in open_analysis:
            ca = open_analysis['command_analysis']
            print(f"\nğŸ’» å‘½ä»¤åˆ†æ:")
            print(f"  å”¯ä¸€å‘½ä»¤æ•°: {ca['unique_commands']}")
            print(f"  å‰20ä¸ªæœ€æ´»è·ƒçš„å‘½ä»¤:")
            for i, (cmd, count) in enumerate(ca['top_commands'].items(), 1):
                print(f"    {i:2d}. {cmd}: {count} æ¬¡")
        
        if 'operation_type_analysis' in open_analysis:
            ota = open_analysis['operation_type_analysis']
            print(f"\nğŸ”§ æ“ä½œç±»å‹åˆ†æ:")
            print(f"  å”¯ä¸€æ“ä½œç±»å‹æ•°: {ota['unique_types']}")
            print(f"  æ“ä½œç±»å‹åˆ†å¸ƒ:")
            for op_type, count in ota['type_distribution'].items():
                print(f"    {op_type}: {count} æ¬¡")
        
        if 'user_analysis' in open_analysis:
            ua = open_analysis['user_analysis']
            print(f"\nğŸ‘¤ ç”¨æˆ·åˆ†æ:")
            print(f"  æ¶‰åŠç”¨æˆ·æ•°: {ua['unique_users']}")
            print(f"  å„ç”¨æˆ·æ“ä½œæ¬¡æ•°:")
            for uid, count in ua['operations_by_uid'].items():
                user_name = "root" if uid == 0 else f"uid_{uid}"
                print(f"    {user_name}: {count} æ¬¡")
        
        if 'failure_analysis' in open_analysis:
            fa = open_analysis['failure_analysis']
            print(f"\nâŒ å¤±è´¥åˆ†æ:")
            print(f"  å¤±è´¥æ¬¡æ•°: {fa['total_failures']}")
            print(f"  å¤±è´¥ç‡: {fa['failure_rate']:.2%}")
            if fa['failed_commands']:
                print(f"  å¤±è´¥æœ€å¤šçš„å‘½ä»¤:")
                for cmd, count in fa['failed_commands'].items():
                    print(f"    {cmd}: {count} æ¬¡")
            if fa['failed_files']:
                print(f"  å¤±è´¥æœ€å¤šçš„æ–‡ä»¶:")
                for filename, count in fa['failed_files'].items():
                    print(f"    {filename}: {count} æ¬¡")
        
        if 'flags_analysis' in open_analysis:
            fla = open_analysis['flags_analysis']
            print(f"\nğŸ æ ‡å¿—ä½åˆ†æ:")
            print(f"  å”¯ä¸€æ ‡å¿—ä½æ•°: {fla['unique_flags']}")
            print(f"  å‰10ä¸ªæœ€å¸¸è§çš„æ ‡å¿—ä½:")
            for i, (flag, count) in enumerate(fla['top_flags'].items(), 1):
                print(f"    {i:2d}. {flag}: {count} æ¬¡")
    
    def print_func_details(self, func_analysis: Dict) -> None:
        """æ‰“å°funcè¯¦ç»†åˆ†æç»“æœ"""
        print("\n" + "="*60)
        print("FUNC ç›‘æ§å™¨è¯¦ç»†åˆ†ææŠ¥å‘Š")
        print("="*60)
        
        if 'function_analysis' in func_analysis:
            fa = func_analysis['function_analysis']
            print(f"\nğŸ”§ å‡½æ•°è°ƒç”¨åˆ†æ:")
            print(f"  æ€»è°ƒç”¨æ¬¡æ•°: {fa['total_calls']}")
            print(f"  å”¯ä¸€å‡½æ•°æ•°: {fa['unique_functions']}")
            print(f"  VFSå‡½æ•°è°ƒç”¨: {fa.get('vfs_calls', 0)} æ¬¡")
            print(f"  SYSå‡½æ•°è°ƒç”¨: {fa.get('sys_calls', 0)} æ¬¡")
            print(f"  å‰20ä¸ªæœ€å¸¸è°ƒç”¨çš„å‡½æ•°:")
            for i, (func, count) in enumerate(fa['top_functions'].items(), 1):
                print(f"    {i:2d}. {func}: {count} æ¬¡")
        
        if 'command_analysis' in func_analysis:
            ca = func_analysis['command_analysis']
            print(f"\nğŸ’» å‘½ä»¤åˆ†æ:")
            print(f"  å”¯ä¸€å‘½ä»¤æ•°: {ca['unique_commands']}")
            print(f"  å‰20ä¸ªæœ€æ´»è·ƒçš„å‘½ä»¤:")
            for i, (cmd, count) in enumerate(ca['top_commands'].items(), 1):
                print(f"    {i:2d}. {cmd}: {count} æ¬¡")
        
        if 'user_analysis' in func_analysis:
            ua = func_analysis['user_analysis']
            print(f"\nğŸ‘¤ ç”¨æˆ·åˆ†æ:")
            print(f"  æ¶‰åŠç”¨æˆ·æ•°: {ua['unique_users']}")
            print(f"  å„ç”¨æˆ·è°ƒç”¨æ¬¡æ•°:")
            for uid, count in ua['calls_by_uid'].items():
                user_name = "root" if uid == 0 else f"uid_{uid}"
                print(f"    {user_name}: {count} æ¬¡")
    
    def print_interrupt_details(self, interrupt_analysis: Dict) -> None:
        """æ‰“å°interruptè¯¦ç»†åˆ†æç»“æœ"""
        print("\n" + "="*60)
        print("INTERRUPT ç›‘æ§å™¨è¯¦ç»†åˆ†ææŠ¥å‘Š")
        print("="*60)
        
        if 'interrupt_type_analysis' in interrupt_analysis:
            ita = interrupt_analysis['interrupt_type_analysis']
            print(f"\nâš¡ ä¸­æ–­ç±»å‹åˆ†æ:")
            print(f"  å”¯ä¸€ä¸­æ–­ç±»å‹æ•°: {ita['unique_types']}")
            print(f"  ä¸­æ–­ç±»å‹åˆ†å¸ƒ:")
            for irq_type, count in ita['type_distribution'].items():
                print(f"    {irq_type}: {count} æ¬¡")
        
        if 'interrupt_name_analysis' in interrupt_analysis:
            ina = interrupt_analysis['interrupt_name_analysis']
            print(f"\nğŸ“› ä¸­æ–­åç§°åˆ†æ:")
            print(f"  å”¯ä¸€ä¸­æ–­åç§°æ•°: {ina['unique_names']}")
            print(f"  å‰20ä¸ªæœ€é¢‘ç¹çš„ä¸­æ–­:")
            for i, (name, count) in enumerate(ina['top_interrupts'].items(), 1):
                print(f"    {i:2d}. {name}: {count} æ¬¡")
        
        if 'duration_analysis' in interrupt_analysis:
            da = interrupt_analysis['duration_analysis']
            print(f"\nâ±ï¸ æŒç»­æ—¶é—´åˆ†æ:")
            print(f"  å¹³å‡æŒç»­æ—¶é—´: {da['avg_duration_us']:.2f} Î¼s")
            print(f"  æœ€å¤§æŒç»­æ—¶é—´: {da['max_duration_us']:.2f} Î¼s")
            print(f"  æœ€å°æŒç»­æ—¶é—´: {da['min_duration_us']:.2f} Î¼s")
            print(f"  æ ‡å‡†å·®: {da['std_duration_us']:.2f} Î¼s")
        
        if 'cpu_analysis' in interrupt_analysis:
            ca = interrupt_analysis['cpu_analysis']
            print(f"\nğŸ–¥ï¸ CPUåˆ†å¸ƒåˆ†æ:")
            for cpu, count in sorted(ca['interrupts_by_cpu'].items()):
                print(f"    CPU {cpu}: {count} æ¬¡ä¸­æ–­")
        
        if 'command_analysis' in interrupt_analysis:
            ca = interrupt_analysis['command_analysis']
            print(f"\nğŸ’» å‘½ä»¤åˆ†æ:")
            print(f"  å”¯ä¸€å‘½ä»¤æ•°: {ca['unique_commands']}")
            print(f"  å‰20ä¸ªæœ€æ´»è·ƒçš„å‘½ä»¤:")
            for i, (cmd, count) in enumerate(ca['top_commands'].items(), 1):
                print(f"    {i:2d}. {cmd}: {count} æ¬¡")
    
    def print_io_details(self, io_analysis: Dict) -> None:
        """æ‰“å°ioè¯¦ç»†åˆ†æç»“æœ"""
        print("\n" + "="*60)
        print("IO ç›‘æ§å™¨è¯¦ç»†åˆ†ææŠ¥å‘Š")
        print("="*60)
        
        if 'io_type_analysis' in io_analysis:
            ita = io_analysis['io_type_analysis']
            print(f"\nğŸ“Š I/Oç±»å‹åˆ†æ:")
            for io_type, count in ita['type_distribution'].items():
                print(f"    {io_type}: {count} æ¬¡")
        
        if 'performance_analysis' in io_analysis:
            pa = io_analysis['performance_analysis']
            print(f"\nğŸš€ æ€§èƒ½åˆ†æ:")
            if 'avg_throughput_mbps' in pa:
                print(f"  å¹³å‡ååé‡: {pa['avg_throughput_mbps']:.2f} MB/s")
                print(f"  æœ€å¤§ååé‡: {pa['max_throughput_mbps']:.2f} MB/s")
                print(f"  æœ€å°ååé‡: {pa['min_throughput_mbps']:.2f} MB/s")
            if 'avg_duration_us' in pa:
                print(f"  å¹³å‡æŒç»­æ—¶é—´: {pa['avg_duration_us']:.2f} Î¼s")
                print(f"  æœ€å¤§æŒç»­æ—¶é—´: {pa['max_duration_us']:.2f} Î¼s")
                print(f"  æœ€å°æŒç»­æ—¶é—´: {pa['min_duration_us']:.2f} Î¼s")
        
        if 'fd_analysis' in io_analysis:
            fa = io_analysis['fd_analysis']
            print(f"\nğŸ“ æ–‡ä»¶æè¿°ç¬¦åˆ†æ:")
            print(f"  å”¯ä¸€æ–‡ä»¶æè¿°ç¬¦æ•°: {fa['unique_fds']}")
            print(f"  å‰10ä¸ªæœ€æ´»è·ƒçš„æ–‡ä»¶æè¿°ç¬¦:")
            for i, (fd, count) in enumerate(fa['top_fds'].items(), 1):
                print(f"    {i:2d}. FD {fd}: {count} æ¬¡")
        
        if 'size_analysis' in io_analysis:
            sa = io_analysis['size_analysis']
            print(f"\nğŸ“ æ•°æ®å¤§å°åˆ†æ:")
            print(f"  å¹³å‡å¤§å°: {sa['avg_size_bytes']:.0f} å­—èŠ‚")
            print(f"  æœ€å¤§å¤§å°: {sa['max_size_bytes']:.0f} å­—èŠ‚")
            print(f"  æ€»æ•°æ®é‡: {sa['total_bytes']:.0f} å­—èŠ‚ ({sa['total_bytes']/1024/1024:.2f} MB)")
        
        if 'command_analysis' in io_analysis:
            ca = io_analysis['command_analysis']
            print(f"\nğŸ’» å‘½ä»¤åˆ†æ:")
            print(f"  å”¯ä¸€å‘½ä»¤æ•°: {ca['unique_commands']}")
            print(f"  å‰20ä¸ªæœ€æ´»è·ƒçš„å‘½ä»¤:")
            for i, (cmd, count) in enumerate(ca['top_commands'].items(), 1):
                print(f"    {i:2d}. {cmd}: {count} æ¬¡")
        
        if 'error_analysis' in io_analysis:
            ea = io_analysis['error_analysis']
            print(f"\nâŒ é”™è¯¯åˆ†æ:")
            print(f"  é”™è¯¯æ¬¡æ•°: {ea['total_errors']}")
            print(f"  é”™è¯¯ç‡: {ea['error_rate']:.2%}")
    
    def print_page_fault_details(self, pf_analysis: Dict) -> None:
        """æ‰“å°page_faultè¯¦ç»†åˆ†æç»“æœ"""
        print("\n" + "="*60)
        print("PAGE_FAULT ç›‘æ§å™¨è¯¦ç»†åˆ†ææŠ¥å‘Š")
        print("="*60)
        
        if 'fault_type_analysis' in pf_analysis:
            fta = pf_analysis['fault_type_analysis']
            print(f"\nğŸ” é¡µé¢é”™è¯¯ç±»å‹åˆ†æ:")
            print(f"  å”¯ä¸€é”™è¯¯ç±»å‹æ•°: {fta['unique_types']}")
            print(f"  é”™è¯¯ç±»å‹åˆ†å¸ƒ:")
            for fault_type, count in fta['type_distribution'].items():
                print(f"    {fault_type}: {count} æ¬¡")
        
        if 'fault_breakdown' in pf_analysis:
            fb = pf_analysis['fault_breakdown']
            print(f"\nğŸ“Š é”™è¯¯åˆ†ç±»ç»Ÿè®¡:")
            for fault_type, count in fb.items():
                fault_name = fault_type.replace('is_', '').replace('_fault', '').replace('_', ' ').title()
                print(f"    {fault_name}: {count} æ¬¡")
        
        if 'address_analysis' in pf_analysis:
            aa = pf_analysis['address_analysis']
            print(f"\nğŸ¯ åœ°å€åˆ†æ:")
            print(f"  æœ€å°åœ°å€: 0x{aa['min_address']:x}")
            print(f"  æœ€å¤§åœ°å€: 0x{aa['max_address']:x}")
            print(f"  å”¯ä¸€åœ°å€æ•°: {aa['unique_addresses']}")
        
        if 'command_analysis' in pf_analysis:
            ca = pf_analysis['command_analysis']
            print(f"\nğŸ’» å‘½ä»¤åˆ†æ:")
            print(f"  å”¯ä¸€å‘½ä»¤æ•°: {ca['unique_commands']}")
            print(f"  å‰20ä¸ªæœ€æ´»è·ƒçš„å‘½ä»¤:")
            for i, (cmd, count) in enumerate(ca['top_commands'].items(), 1):
                print(f"    {i:2d}. {cmd}: {count} æ¬¡")
        
        if 'cpu_analysis' in pf_analysis:
            ca = pf_analysis['cpu_analysis']
            print(f"\nğŸ–¥ï¸ CPUåˆ†å¸ƒåˆ†æ:")
            for cpu, count in sorted(ca['faults_by_cpu'].items()):
                print(f"    CPU {cpu}: {count} æ¬¡é¡µé¢é”™è¯¯")
    
    def print_syscall_details(self, syscall_analysis: Dict) -> None:
        """æ‰“å°syscallè¯¦ç»†åˆ†æç»“æœ"""
        print("\n" + "="*60)
        print("SYSCALL ç›‘æ§å™¨è¯¦ç»†åˆ†ææŠ¥å‘Š")
        print("="*60)
        
        if 'syscall_analysis' in syscall_analysis:
            sa = syscall_analysis['syscall_analysis']
            print(f"\nğŸ”§ ç³»ç»Ÿè°ƒç”¨åˆ†æ:")
            print(f"  æ€»è°ƒç”¨æ¬¡æ•°: {sa['total_calls']}")
            print(f"  å”¯ä¸€ç³»ç»Ÿè°ƒç”¨æ•°: {sa['unique_syscalls']}")
            print(f"  å‰20ä¸ªæœ€å¸¸ç”¨çš„ç³»ç»Ÿè°ƒç”¨:")
            for i, (syscall, count) in enumerate(sa['top_syscalls'].items(), 1):
                print(f"    {i:2d}. {syscall}: {count} æ¬¡")
        
        if 'category_analysis' in syscall_analysis:
            ca = syscall_analysis['category_analysis']
            print(f"\nğŸ“‚ ç±»åˆ«åˆ†æ:")
            print(f"  å”¯ä¸€ç±»åˆ«æ•°: {ca['unique_categories']}")
            print(f"  ç±»åˆ«åˆ†å¸ƒ:")
            for category, count in ca['category_distribution'].items():
                print(f"    {category}: {count} æ¬¡")
        
        if 'performance_analysis' in syscall_analysis:
            pa = syscall_analysis['performance_analysis']
            print(f"\nâ±ï¸ æ€§èƒ½åˆ†æ:")
            print(f"  å¹³å‡æŒç»­æ—¶é—´: {pa['avg_duration_ms']:.4f} ms")
            print(f"  æœ€å¤§æŒç»­æ—¶é—´: {pa['max_duration_ms']:.4f} ms")
            print(f"  æœ€å°æŒç»­æ—¶é—´: {pa['min_duration_ms']:.4f} ms")
            print(f"  æ ‡å‡†å·®: {pa['std_duration_ms']:.4f} ms")
        
        if 'error_analysis' in syscall_analysis:
            ea = syscall_analysis['error_analysis']
            print(f"\nâŒ é”™è¯¯åˆ†æ:")
            print(f"  é”™è¯¯æ¬¡æ•°: {ea['total_errors']}")
            print(f"  é”™è¯¯ç‡: {ea['error_rate']:.2%}")
            if 'error_types' in ea:
                print(f"  é”™è¯¯ç±»å‹åˆ†å¸ƒ:")
                for error_type, count in ea['error_types'].items():
                    print(f"    {error_type}: {count} æ¬¡")
        
        if 'slow_call_analysis' in syscall_analysis:
            sca = syscall_analysis['slow_call_analysis']
            print(f"\nğŸŒ æ…¢è°ƒç”¨åˆ†æ:")
            print(f"  æ…¢è°ƒç”¨æ¬¡æ•°: {sca['total_slow_calls']}")
            print(f"  æ…¢è°ƒç”¨ç‡: {sca['slow_call_rate']:.2%}")
        
        if 'command_analysis' in syscall_analysis:
            ca = syscall_analysis['command_analysis']
            print(f"\nğŸ’» å‘½ä»¤åˆ†æ:")
            print(f"  å”¯ä¸€å‘½ä»¤æ•°: {ca['unique_commands']}")
            print(f"  å‰20ä¸ªæœ€æ´»è·ƒçš„å‘½ä»¤:")
            for i, (cmd, count) in enumerate(ca['top_commands'].items(), 1):
                print(f"    {i:2d}. {cmd}: {count} æ¬¡")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='eBPFæ•°æ®åˆ†æå·¥å…·')
    parser.add_argument('--analyze', type=str, help='åˆ†ææŒ‡å®šæ—¥æœŸçš„æ•°æ® (YYYYMMDDæ ¼å¼)')
    parser.add_argument('--analyze-exec', type=str, help='è¯¦ç»†åˆ†ææŒ‡å®šæ—¥æœŸçš„execæ•°æ® (YYYYMMDDæ ¼å¼)')
    parser.add_argument('--analyze-open', type=str, help='è¯¦ç»†åˆ†ææŒ‡å®šæ—¥æœŸçš„openæ•°æ® (YYYYMMDDæ ¼å¼)')
    parser.add_argument('--analyze-func', type=str, help='è¯¦ç»†åˆ†ææŒ‡å®šæ—¥æœŸçš„funcæ•°æ® (YYYYMMDDæ ¼å¼)')
    parser.add_argument('--analyze-interrupt', type=str, help='è¯¦ç»†åˆ†ææŒ‡å®šæ—¥æœŸçš„interruptæ•°æ® (YYYYMMDDæ ¼å¼)')
    parser.add_argument('--analyze-io', type=str, help='è¯¦ç»†åˆ†ææŒ‡å®šæ—¥æœŸçš„ioæ•°æ® (YYYYMMDDæ ¼å¼)')
    parser.add_argument('--analyze-page-fault', type=str, help='è¯¦ç»†åˆ†ææŒ‡å®šæ—¥æœŸçš„page_faultæ•°æ® (YYYYMMDDæ ¼å¼)')
    parser.add_argument('--analyze-syscall', type=str, help='è¯¦ç»†åˆ†ææŒ‡å®šæ—¥æœŸçš„syscallæ•°æ® (YYYYMMDDæ ¼å¼)')
    parser.add_argument('--compare', nargs='+', help='å¯¹æ¯”å¤šä¸ªæ—¥æœŸçš„æ•°æ®')
    parser.add_argument('--date-range', nargs=2, metavar=('START', 'END'), 
                       help='åˆ†ææ—¥æœŸèŒƒå›´å†…çš„æ•°æ®')
    parser.add_argument('--monitors', nargs='+', 
                       choices=['exec', 'syscall', 'io', 'interrupt', 'func', 'open', 'page_fault'],
                       help='æŒ‡å®šç›‘æ§å™¨ç±»å‹')
    parser.add_argument('--list-dates', action='store_true', help='åˆ—å‡ºå¯ç”¨çš„æ—¥æœŸ')
    parser.add_argument('--output-dir', default='../output', help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--daily-dir', default='./daily_data', help='æ—¥æ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--verbose', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    analyzer = EBPFAnalyzer(args.output_dir, args.daily_dir)
    
    if args.list_dates:
        dates = analyzer.get_available_dates()
        print("å¯ç”¨æ—¥æœŸ:")
        for date in dates:
            print(f"  {date}")
    
    elif args.analyze:
        data = analyzer.load_daily_data(args.analyze, args.monitors)
        if data:
            performance = analyzer.analyze_performance(data)
            results = {args.analyze: performance}
            analyzer.print_summary(results)
            
            # å¼‚å¸¸æ£€æµ‹
            anomalies = analyzer.detect_anomalies(data)
            if anomalies:
                print(f"\næ£€æµ‹åˆ°å¼‚å¸¸:")
                for anomaly_type, anomaly_list in anomalies.items():
                    print(f"  {anomaly_type}: {len(anomaly_list)} ä¸ªå¼‚å¸¸")
        else:
            print(f"æœªæ‰¾åˆ°æ—¥æœŸ {args.analyze} çš„æ•°æ®")
    
    elif args.analyze_exec:
        data = analyzer.load_daily_data(args.analyze_exec, ['exec'])
        if data:
            exec_analysis = analyzer.analyze_exec_details(data)
            analyzer.print_exec_details(exec_analysis)
        else:
            print(f"æœªæ‰¾åˆ°æ—¥æœŸ {args.analyze_exec} çš„execæ•°æ®")
    
    elif args.analyze_open:
        data = analyzer.load_daily_data(args.analyze_open, ['open'])
        if data:
            open_analysis = analyzer.analyze_open_details(data)
            analyzer.print_open_details(open_analysis)
        else:
            print(f"æœªæ‰¾åˆ°æ—¥æœŸ {args.analyze_open} çš„openæ•°æ®")
    
    elif args.analyze_func:
        data = analyzer.load_daily_data(args.analyze_func, ['func'])
        if data:
            func_analysis = analyzer.analyze_func_details(data)
            analyzer.print_func_details(func_analysis)
        else:
            print(f"æœªæ‰¾åˆ°æ—¥æœŸ {args.analyze_func} çš„funcæ•°æ®")
    
    elif args.analyze_interrupt:
        data = analyzer.load_daily_data(args.analyze_interrupt, ['interrupt'])
        if data:
            interrupt_analysis = analyzer.analyze_interrupt_details(data)
            analyzer.print_interrupt_details(interrupt_analysis)
        else:
            print(f"æœªæ‰¾åˆ°æ—¥æœŸ {args.analyze_interrupt} çš„interruptæ•°æ®")
    
    elif args.analyze_io:
        data = analyzer.load_daily_data(args.analyze_io, ['io'])
        if data:
            io_analysis = analyzer.analyze_io_details(data)
            analyzer.print_io_details(io_analysis)
        else:
            print(f"æœªæ‰¾åˆ°æ—¥æœŸ {args.analyze_io} çš„ioæ•°æ®")
    
    elif args.analyze_page_fault:
        data = analyzer.load_daily_data(args.analyze_page_fault, ['page_fault'])
        if data:
            pf_analysis = analyzer.analyze_page_fault_details(data)
            analyzer.print_page_fault_details(pf_analysis)
        else:
            print(f"æœªæ‰¾åˆ°æ—¥æœŸ {args.analyze_page_fault} çš„page_faultæ•°æ®")
    
    elif args.analyze_syscall:
        data = analyzer.load_daily_data(args.analyze_syscall, ['syscall'])
        if data:
            syscall_analysis = analyzer.analyze_syscall_details(data)
            analyzer.print_syscall_details(syscall_analysis)
        else:
            print(f"æœªæ‰¾åˆ°æ—¥æœŸ {args.analyze_syscall} çš„syscallæ•°æ®")
    
    elif args.compare:
        results = analyzer.compare_systems(args.compare, args.monitors)
        analyzer.print_summary(results)
    
    elif args.date_range:
        start_date, end_date = args.date_range
        data_by_date = analyzer.load_date_range(start_date, end_date, args.monitors)
        
        # åˆ†ææ¯ä¸€å¤©çš„æ•°æ®
        results = {}
        for date, daily_data in data_by_date.items():
            performance = analyzer.analyze_performance(daily_data)
            results[date] = performance
        
        analyzer.print_summary(results)
    
    else:
        parser.print_help()


if __name__ == '__main__':
    main()
